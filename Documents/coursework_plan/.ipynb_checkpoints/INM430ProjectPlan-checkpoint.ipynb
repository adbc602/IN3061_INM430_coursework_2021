{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3061/INM430 - Tiny DS Project Progress Report\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "** Student Name: ** _.... Write your name here Andrea Cortes..._\n",
    "\n",
    "** Project Title:** _\n",
    "\n",
    "“Analyzing the Breast Cancer Data for gathering insights and developing a diagnostic strategy to detect it at early stage”_\n",
    "\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application domain and datasets (150 words):\n",
    "\n",
    "Which application domain and which datasets?\n",
    "\n",
    "\n",
    "\n",
    "This application belongs to Healthcare Domain as it is the one of the most common type of cancer that occur more frequently in females as compared to males. According to a research, every 3 out of 9 females are suffering with this deadliest disease. There will be a good survival rate if it is predicted at an earlier stage. For that I am using Breast Cancer Wisconsin (Diagnostic) Data Set available at UCI Machine Learning Repository. The dataset is a free source and can be downloaded for the analysis and making models for the predictions. The dataset contain several independent features on the basis of each tumor (cancerous cell) is labelled to be benign or malignant. This is the target column to be predicted.\n",
    "\n",
    "\n",
    "\n",
    "## Well-motivated analytical questions (150 words):\n",
    "\n",
    "What are your analytical questions and what is your motivation for answering them?\n",
    "\n",
    "\n",
    "\n",
    "The dataset has several independent attributes that are used to predict the target label. The intent to choose this problem is to find the correlation among different attributes and to get insights from the data. That what are the attributes that are important and should be helpful in the efficient model building and taking the predictions form that trained model. Which attributes are the key attributes and which of them should be skipped and there will be no effect on the data so these are the analytics that need to be performed on the dataset. Analyzing the data frame using only the numerical attributes and check for the accuracy of the model predictions and after taking all the attributes and check whether the performance is enhanced or not.\n",
    "\n",
    "***\n",
    "\n",
    "## Plan (maximum 200 words): \n",
    "\n",
    "What is your plan? Include data processing, data derivation, model building and validation. The aim of your plan is to address your research questions.\n",
    "\n",
    "The plan has to be implemented in several steps:\n",
    "\t•\tTake the dataset and identify the attributes and their meaning what they are presenting, we can say it as gathering the domain knowledge.\n",
    "\t•\tThe next step is the data preprocessing. Analyze the data attributes, removing the redundant or misleading attributes, identifying the null values and replace them with the standard value based on attribute, converting the categorical attributes to the numerical one to make data efficient to build the model.\n",
    "\t•\tThe next step is the most important which is known as Exploratory Data Analysis (EDA). In this step the hidden insights from the dataset. The data can be represented in the form of visualizations that speaks the hidden information from the data.\n",
    "\t•\tAfter making the data feasible to make a model, as it’s a supervised learning problem (i.e. having the target labels) so we will build classification models and train them on the training data.\n",
    "\t•\tAfter the model is trained and ready to provide us output, the output is providing the predictions on the test data. (i.e. predicting the target variable for the unseen data).\n",
    "\t•\tThe last step is to use several performance measures to know how well the model is trained. It could be accuracy, precision, recall and confusion matrix and classification report.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Specific questions to us\n",
    "\n",
    "We will give general comments on your approach, but our feedback will be more useful if you have some more specific questions.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Initial investigations\n",
    "\n",
    "I have started the project by parsing the data csv file that is downloaded from the UCI Machine Learning Repository into a python data frame using pandas. After that visualize the head of the data frame. Checking the shape of the data frame so there are 33 columns and 569 rows in the data frame. Now I start moving further as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
